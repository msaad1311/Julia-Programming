{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Flux: Data.DataLoader\n",
    "using Flux: onehotbatch,onecold,crossentropy\n",
    "using Flux: @epochs\n",
    "using Statistics\n",
    "using MLDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_temp,y_train_temp = MNIST.traindata();\n",
    "x_test_temp,y_test_temp = MNIST.testdata();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the training images are (28, 28, 60000) and the size of training labels is (60000,)\n",
      "The size of the testing images are (28, 28, 10000) and the size of testing labels is (10000,)"
     ]
    }
   ],
   "source": [
    "println(\"The size of the training images are $(size(x_train_temp)) and the size of training labels is $(size(y_train_temp))\")\n",
    "print(\"The size of the testing images are $(size(x_test_temp)) and the size of testing labels is $(size(y_test_temp))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the training images are (28, 28, 1, 1000) and the size of training labels is (10, 1000)\n",
      "The size of the testing images are (28, 28, 1, 10000) and the size of testing labels is (10, 10000)"
     ]
    }
   ],
   "source": [
    "x_train = Flux.unsqueeze(x_train_temp[:,:,1:1000],3)\n",
    "x_test = Flux.unsqueeze(x_test_temp,3)\n",
    "\n",
    "y_train = onehotbatch(y_train_temp[1:1000],0:9)\n",
    "y_test = onehotbatch(y_test_temp,0:9)\n",
    "\n",
    "#Displaying the updated shape of the elements\n",
    "\n",
    "println(\"The size of the training images are $(size(x_train)) and the size of training labels is $(size(y_train))\")\n",
    "print(\"The size of the testing images are $(size(x_test)) and the size of testing labels is $(size(y_test))\")\n",
    "\n",
    "train_data = DataLoader(x_train, y_train, batchsize=128);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = Conv((3,3),1=>8,relu,stride=2)\n",
    "layer2 = Conv((3,3),8=>16,relu)\n",
    "layer3 = Conv((3,3),16=>32,relu)\n",
    "layer4 = GlobalMaxPool()\n",
    "layer5 = flatten\n",
    "layer6 = Dense(32,10)\n",
    "layer7 = softmax;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The layer is Conv((3, 3), 1=>8, relu)\n",
      "The size of the input layer is (28, 28, 1, 1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Slow fallback implementation invoked for conv!  You probably don't want this; check your datatypes.\n",
      "│   yT = Float32\n",
      "│   T1 = FixedPointNumbers.Normed{UInt8,8}\n",
      "│   T2 = Float32\n",
      "└ @ NNlib C:\\Users\\Saad.LAKES\\.julia\\packages\\NNlib\\fxLrD\\src\\conv.jl:206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the output layer is (13, 13, 8, 1000)\n",
      "------------------------------\n",
      "The layer is Conv((3, 3), 8=>16, relu)\n",
      "The size of the input layer is (13, 13, 8, 1000)\n",
      "The size of the output layer is (11, 11, 16, 1000)\n",
      "------------------------------\n",
      "The layer is Conv((3, 3), 16=>32, relu)\n",
      "The size of the input layer is (11, 11, 16, 1000)\n",
      "The size of the output layer is (9, 9, 32, 1000)\n",
      "------------------------------\n",
      "The layer is GlobalMaxPool()\n",
      "The size of the input layer is (9, 9, 32, 1000)\n",
      "The size of the output layer is (1, 1, 32, 1000)\n",
      "------------------------------\n",
      "The layer is flatten\n",
      "The size of the input layer is (1, 1, 32, 1000)\n",
      "The size of the output layer is (32, 1000)\n",
      "------------------------------\n",
      "The layer is Dense(32, 10)\n",
      "The size of the input layer is (32, 1000)\n",
      "The size of the output layer is (10, 1000)\n",
      "------------------------------\n",
      "The layer is softmax\n",
      "The size of the input layer is (10, 1000)\n",
      "The size of the output layer is (10, 1000)\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "layers = [layer1, layer2, layer3, layer4, layer5, layer6, layer7]\n",
    "x = x_train\n",
    "\n",
    "for l in layers\n",
    "    println(\"The layer is $l\")\n",
    "    println(\"The size of the input layer is $(size(x))\")\n",
    "    x = l(x)\n",
    "    println(\"The size of the output layer is $(size(x))\")\n",
    "    println(\"-\"^30)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model = Chain(layer1, layer2, layer3, layer4, layer5,layer6, layer7)\n",
    "# # model = Chain(Dense(784,32),Dense(32,10),softmax)\n",
    "# L(x,y) = Flux.crossentropy(model(x),y)\n",
    "# opt = Flux.Optimise.ADAM()\n",
    "# ps = Flux.params(model)\n",
    "\n",
    "# evalcb() = @show(L(x_train, y_train))\n",
    "# throttled_cb = Flux.throttle(evalcb, 1)\n",
    "\n",
    "# # callback() = push!(loss_vector, L(x_train, y_train))\n",
    "# # loss_vector = Vector{Float32}()\n",
    "# print(\"Training started\")\n",
    "\n",
    "# @epochs 100 Flux.train!(L,ps,train_data,opt, cb = throttled_cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH NUMBER: 1\n",
      "Training Loss is 0.05334028\n",
      "Validation Loss is 0.41607904\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 2\n",
      "Training Loss is 0.033022523\n",
      "Validation Loss is 0.43631163\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 3\n",
      "Training Loss is 0.019734679\n",
      "Validation Loss is 0.39910105\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 4\n",
      "Training Loss is 0.020999355\n",
      "Validation Loss is 0.40572324\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 5\n",
      "Training Loss is 0.017284023\n",
      "Validation Loss is 0.41206014\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 6\n",
      "Training Loss is 0.01517956\n",
      "Validation Loss is 0.4153704\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 7\n",
      "Training Loss is 0.015405611\n",
      "Validation Loss is 0.40452024\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 8\n",
      "Training Loss is 0.013933814\n",
      "Validation Loss is 0.42643934\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 9\n",
      "Training Loss is 0.012007096\n",
      "Validation Loss is 0.40339103\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 10\n",
      "Training Loss is 0.012175408\n",
      "Validation Loss is 0.43285343\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 11\n",
      "Training Loss is 0.011604154\n",
      "Validation Loss is 0.4086899\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 12\n",
      "Training Loss is 0.0121366875\n",
      "Validation Loss is 0.4474719\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 13\n",
      "Training Loss is 0.01095062\n",
      "Validation Loss is 0.41473854\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 14\n",
      "Training Loss is 0.012077069\n",
      "Validation Loss is 0.4601402\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 15\n",
      "Training Loss is 0.010796067\n",
      "Validation Loss is 0.4225555\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 16\n",
      "Training Loss is 0.011885781\n",
      "Validation Loss is 0.47102216\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 17\n",
      "Training Loss is 0.010533804\n",
      "Validation Loss is 0.4296888\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 18\n",
      "Training Loss is 0.011751384\n",
      "Validation Loss is 0.48191744\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 19\n",
      "Training Loss is 0.010602179\n",
      "Validation Loss is 0.438536\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 20\n",
      "Training Loss is 0.012321321\n",
      "Validation Loss is 0.4953537\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 21\n",
      "Training Loss is 0.010810579\n",
      "Validation Loss is 0.4469837\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 22\n",
      "Training Loss is 0.012706498\n",
      "Validation Loss is 0.50821805\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 23\n",
      "Training Loss is 0.011295451\n",
      "Validation Loss is 0.45703578\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 24\n",
      "Training Loss is 0.013553737\n",
      "Validation Loss is 0.52328587\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 25\n",
      "Training Loss is 0.011830261\n",
      "Validation Loss is 0.46716768\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 26\n",
      "Training Loss is 0.015048154\n",
      "Validation Loss is 0.54024094\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 27\n",
      "Training Loss is 0.012632925\n",
      "Validation Loss is 0.4775467\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 28\n",
      "Training Loss is 0.016359864\n",
      "Validation Loss is 0.55479425\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 29\n",
      "Training Loss is 0.013676312\n",
      "Validation Loss is 0.49029434\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 30\n",
      "Training Loss is 0.01858631\n",
      "Validation Loss is 0.5755366\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 31\n",
      "Training Loss is 0.015600375\n",
      "Validation Loss is 0.5039417\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 32\n",
      "Training Loss is 0.020236872\n",
      "Validation Loss is 0.58914495\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 33\n",
      "Training Loss is 0.016782349\n",
      "Validation Loss is 0.51774436\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 34\n",
      "Training Loss is 0.025545238\n",
      "Validation Loss is 0.62051386\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 35\n",
      "Training Loss is 0.01721755\n",
      "Validation Loss is 0.5282956\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 36\n",
      "Training Loss is 0.03196467\n",
      "Validation Loss is 0.6386501\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 37\n",
      "Training Loss is 0.021871435\n",
      "Validation Loss is 0.5508476\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 38\n",
      "Training Loss is 0.02990741\n",
      "Validation Loss is 0.6469249\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 39\n",
      "Training Loss is 0.02846761\n",
      "Validation Loss is 0.5779926\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 40\n",
      "Training Loss is 0.027883938\n",
      "Validation Loss is 0.6637995\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 41\n",
      "Training Loss is 0.030937953\n",
      "Validation Loss is 0.6010292\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 42\n",
      "Training Loss is 0.021219037\n",
      "Validation Loss is 0.62881786\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 43\n",
      "Training Loss is 0.023633769\n",
      "Validation Loss is 0.5661413\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 44\n",
      "Training Loss is 0.0075573153\n",
      "Validation Loss is 0.5559995\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 45\n",
      "Training Loss is 0.043913975\n",
      "Validation Loss is 0.655643\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 46\n",
      "Training Loss is 0.0196746\n",
      "Validation Loss is 0.62829256\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 47\n",
      "Training Loss is 0.010441322\n",
      "Validation Loss is 0.5532416\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 48\n",
      "Training Loss is 0.029300325\n",
      "Validation Loss is 0.6324213\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 49\n",
      "Training Loss is 0.012395123\n",
      "Validation Loss is 0.5732724\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 50\n",
      "Training Loss is 0.03056314\n",
      "Validation Loss is 0.670622\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 51\n",
      "Training Loss is 0.08820338\n",
      "Validation Loss is 0.7872331\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 52\n",
      "Training Loss is 0.014401666\n",
      "Validation Loss is 0.5843723\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 53\n",
      "Training Loss is 0.0115310745\n",
      "Validation Loss is 0.57145125\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 54\n",
      "Training Loss is 0.0098337615\n",
      "Validation Loss is 0.5250956\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 55\n",
      "Training Loss is 0.03999158\n",
      "Validation Loss is 0.65958846\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 56\n",
      "Training Loss is 0.038395714\n",
      "Validation Loss is 0.6578081\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 57\n",
      "Training Loss is 0.07021334\n",
      "Validation Loss is 0.71870273\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 58\n",
      "Training Loss is 0.027768511\n",
      "Validation Loss is 0.61866426\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 59\n",
      "Training Loss is 0.031490605\n",
      "Validation Loss is 0.5665586\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 60\n",
      "Training Loss is 0.010809615\n",
      "Validation Loss is 0.5158698\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 61\n",
      "Training Loss is 0.0045127296\n",
      "Validation Loss is 0.49502802\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 62\n",
      "Training Loss is 0.0050660428\n",
      "Validation Loss is 0.49476483\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 63\n",
      "Training Loss is 0.0027201625\n",
      "Validation Loss is 0.47084004\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 64\n",
      "Training Loss is 0.002064769\n",
      "Validation Loss is 0.46379766\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 65\n",
      "Training Loss is 0.0018975253\n",
      "Validation Loss is 0.46401525\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH NUMBER: 66\n",
      "Training Loss is 0.0017542881\n",
      "Validation Loss is 0.46449238\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 67\n",
      "Training Loss is 0.0016138303\n",
      "Validation Loss is 0.46519887\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 68\n",
      "Training Loss is 0.0015471834\n",
      "Validation Loss is 0.46548364\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 69\n",
      "Training Loss is 0.001499524\n",
      "Validation Loss is 0.46451998\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 70\n",
      "Training Loss is 0.0014587882\n",
      "Validation Loss is 0.46361822\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 71\n",
      "Training Loss is 0.0014248887\n",
      "Validation Loss is 0.46363208\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 72\n",
      "Training Loss is 0.0013936943\n",
      "Validation Loss is 0.46416163\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 73\n",
      "Training Loss is 0.0013647692\n",
      "Validation Loss is 0.46468183\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 74\n",
      "Training Loss is 0.0013374753\n",
      "Validation Loss is 0.46508378\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 75\n",
      "Training Loss is 0.0013116653\n",
      "Validation Loss is 0.46547803\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 76\n",
      "Training Loss is 0.0012872199\n",
      "Validation Loss is 0.46591377\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 77\n",
      "Training Loss is 0.0012639054\n",
      "Validation Loss is 0.4663518\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 78\n",
      "Training Loss is 0.0012416821\n",
      "Validation Loss is 0.46675947\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 79\n",
      "Training Loss is 0.0012203058\n",
      "Validation Loss is 0.4671083\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 80\n",
      "Training Loss is 0.0011996111\n",
      "Validation Loss is 0.467462\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 81\n",
      "Training Loss is 0.0011796003\n",
      "Validation Loss is 0.4678611\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 82\n",
      "Training Loss is 0.0011603786\n",
      "Validation Loss is 0.4682797\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 83\n",
      "Training Loss is 0.0011418905\n",
      "Validation Loss is 0.46868056\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 84\n",
      "Training Loss is 0.0011240031\n",
      "Validation Loss is 0.46907783\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 85\n",
      "Training Loss is 0.0011066506\n",
      "Validation Loss is 0.46946925\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 86\n",
      "Training Loss is 0.0010898383\n",
      "Validation Loss is 0.4698395\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 87\n",
      "Training Loss is 0.0010736126\n",
      "Validation Loss is 0.47021744\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 88\n",
      "Training Loss is 0.0010579246\n",
      "Validation Loss is 0.47063106\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 89\n",
      "Training Loss is 0.0010426535\n",
      "Validation Loss is 0.47104335\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 90\n",
      "Training Loss is 0.0010277921\n",
      "Validation Loss is 0.4714468\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 91\n",
      "Training Loss is 0.0010133743\n",
      "Validation Loss is 0.47183448\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 92\n",
      "Training Loss is 0.000999351\n",
      "Validation Loss is 0.47222564\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 93\n",
      "Training Loss is 0.000985658\n",
      "Validation Loss is 0.47262147\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 94\n",
      "Training Loss is 0.00097233703\n",
      "Validation Loss is 0.47302443\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 95\n",
      "Training Loss is 0.00095937174\n",
      "Validation Loss is 0.47339526\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 96\n",
      "Training Loss is 0.0009467553\n",
      "Validation Loss is 0.47378486\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 97\n",
      "Training Loss is 0.0009344732\n",
      "Validation Loss is 0.47420073\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 98\n",
      "Training Loss is 0.0009225819\n",
      "Validation Loss is 0.47462592\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 99\n",
      "Training Loss is 0.00091098313\n",
      "Validation Loss is 0.47503847\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 100\n",
      "Training Loss is 0.00089961407\n",
      "Validation Loss is 0.47544828\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model1 = Chain(layer1, layer2, layer3, layer4, layer5,layer6, layer7)\n",
    "L1(x,y) = Flux.crossentropy(model1(x),y)\n",
    "opt1 = Flux.Optimise.ADAM()\n",
    "ps1 = Flux.params(model1)\n",
    "\n",
    "total_step = 100\n",
    "\n",
    "for step in 1:total_step\n",
    "    Flux.train!(L1,ps1,train_data,opt1)\n",
    "    println(\"EPOCH NUMBER: $step\")\n",
    "    println(\"Training Loss is $(L1(x_train,y_train))\")\n",
    "    println(\"Validation Loss is $(L1(x_test,y_test))\")\n",
    "    println(\"-\"^50)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(model(x_test),y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo=reshape(x_train_temp,(784,60000));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28, 60000)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size(x_train_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching vec(::Float64)\nClosest candidates are:\n  vec(!Matched::LinearAlgebra.Transpose{T,var\"#s828\"} where var\"#s828\"<:(AbstractArray{T,1} where T) where T) at C:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.5\\LinearAlgebra\\src\\adjtrans.jl:226\n  vec(!Matched::LinearAlgebra.Adjoint{var\"#s828\",var\"#s8281\"} where var\"#s8281\"<:(AbstractArray{T,1} where T) where var\"#s828\"<:Real) at C:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.5\\LinearAlgebra\\src\\adjtrans.jl:227\n  vec(!Matched::SparseArrays.AbstractSparseArray{Tv,Ti,1} where Ti where Tv) at C:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.5\\SparseArrays\\src\\sparsevector.jl:911\n  ...",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching vec(::Float64)\nClosest candidates are:\n  vec(!Matched::LinearAlgebra.Transpose{T,var\"#s828\"} where var\"#s828\"<:(AbstractArray{T,1} where T) where T) at C:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.5\\LinearAlgebra\\src\\adjtrans.jl:226\n  vec(!Matched::LinearAlgebra.Adjoint{var\"#s828\",var\"#s8281\"} where var\"#s8281\"<:(AbstractArray{T,1} where T) where var\"#s828\"<:Real) at C:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.5\\LinearAlgebra\\src\\adjtrans.jl:227\n  vec(!Matched::SparseArrays.AbstractSparseArray{Tv,Ti,1} where Ti where Tv) at C:\\buildbot\\worker\\package_win64\\build\\usr\\share\\julia\\stdlib\\v1.5\\SparseArrays\\src\\sparsevector.jl:911\n  ...",
      "",
      "Stacktrace:",
      " [1] preprocess(::FixedPointNumbers.Normed{UInt8,8}) at .\\In[120]:1",
      " [2] _broadcast_getindex_evalf at .\\broadcast.jl:648 [inlined]",
      " [3] _broadcast_getindex at .\\broadcast.jl:621 [inlined]",
      " [4] getindex at .\\broadcast.jl:575 [inlined]",
      " [5] copy at .\\broadcast.jl:876 [inlined]",
      " [6] materialize(::Base.Broadcast.Broadcasted{Base.Broadcast.DefaultArrayStyle{1},Nothing,typeof(preprocess),Tuple{Array{FixedPointNumbers.Normed{UInt8,8},1}}}) at .\\broadcast.jl:837",
      " [7] top-level scope at In[120]:2",
      " [8] include_string(::Function, ::Module, ::String, ::String) at .\\loading.jl:1091"
     ]
    }
   ],
   "source": [
    "preprocess(p) = vec(Float64.(p))\n",
    "foo = preprocess.(x_train_temp[1:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28×28×60000 Array{Int64,3}:\n",
       "[:, :, 1] =\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " ⋮              ⋮              ⋮        ⋱              ⋮              ⋮     \n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       "\n",
       "[:, :, 2] =\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " ⋮              ⋮              ⋮        ⋱              ⋮              ⋮     \n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       "\n",
       "[:, :, 3] =\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " ⋮              ⋮              ⋮        ⋱              ⋮              ⋮     \n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       "\n",
       "...\n",
       "\n",
       "[:, :, 59998] =\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " ⋮              ⋮              ⋮        ⋱              ⋮              ⋮     \n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       "\n",
       "[:, :, 59999] =\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " ⋮              ⋮              ⋮        ⋱              ⋮              ⋮     \n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       "\n",
       "[:, :, 60000] =\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " ⋮              ⋮              ⋮        ⋱              ⋮              ⋮     \n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length.(x_train_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
