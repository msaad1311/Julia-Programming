{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Flux: Data.DataLoader\n",
    "using Flux: onehotbatch,onecold,crossentropy\n",
    "using Flux: @epochs\n",
    "using Statistics\n",
    "using MLDatasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_temp,y_train_temp = MNIST.traindata();\n",
    "x_test_temp,y_test_temp = MNIST.testdata();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the training images are (28, 28, 60000) and the size of training labels is (60000,)\n",
      "The size of the testing images are (28, 28, 10000) and the size of testing labels is (10000,)"
     ]
    }
   ],
   "source": [
    "println(\"The size of the training images are $(size(x_train_temp)) and the size of training labels is $(size(y_train_temp))\")\n",
    "print(\"The size of the testing images are $(size(x_test_temp)) and the size of testing labels is $(size(y_test_temp))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the training images are (28, 28, 1, 1000) and the size of training labels is (10, 1000)\n",
      "The size of the testing images are (28, 28, 1, 10000) and the size of testing labels is (10, 10000)"
     ]
    }
   ],
   "source": [
    "x_train = Flux.unsqueeze(x_train_temp[:,:,1:5000],3)\n",
    "x_test = Flux.unsqueeze(x_test_temp[1:1000],3)\n",
    "\n",
    "y_train = onehotbatch(y_train_temp[1:5000],0:9)\n",
    "y_test = onehotbatch(y_test_temp[1:1000],0:9)\n",
    "\n",
    "#Displaying the updated shape of the elements\n",
    "\n",
    "println(\"The size of the training images are $(size(x_train)) and the size of training labels is $(size(y_train))\")\n",
    "print(\"The size of the testing images are $(size(x_test)) and the size of testing labels is $(size(y_test))\")\n",
    "\n",
    "train_data = DataLoader(x_train, y_train, batchsize=128);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = Conv((3,3),1=>8,relu,stride=2)\n",
    "layer2 = Conv((3,3),8=>16,relu)\n",
    "layer3 = Conv((3,3),16=>32,relu)\n",
    "layer4 = GlobalMaxPool()\n",
    "layer5 = flatten\n",
    "layer6 = Dense(32,10)\n",
    "layer7 = softmax;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The layer is Conv((3, 3), 1=>8, relu)\n",
      "The size of the input layer is (28, 28, 1, 1000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: Slow fallback implementation invoked for conv!  You probably don't want this; check your datatypes.\n",
      "│   yT = Float32\n",
      "│   T1 = FixedPointNumbers.Normed{UInt8,8}\n",
      "│   T2 = Float32\n",
      "└ @ NNlib C:\\Users\\Saad.LAKES\\.julia\\packages\\NNlib\\fxLrD\\src\\conv.jl:206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the output layer is (13, 13, 8, 1000)\n",
      "------------------------------\n",
      "The layer is Conv((3, 3), 8=>16, relu)\n",
      "The size of the input layer is (13, 13, 8, 1000)\n",
      "The size of the output layer is (11, 11, 16, 1000)\n",
      "------------------------------\n",
      "The layer is Conv((3, 3), 16=>32, relu)\n",
      "The size of the input layer is (11, 11, 16, 1000)\n",
      "The size of the output layer is (9, 9, 32, 1000)\n",
      "------------------------------\n",
      "The layer is GlobalMaxPool()\n",
      "The size of the input layer is (9, 9, 32, 1000)\n",
      "The size of the output layer is (1, 1, 32, 1000)\n",
      "------------------------------\n",
      "The layer is flatten\n",
      "The size of the input layer is (1, 1, 32, 1000)\n",
      "The size of the output layer is (32, 1000)\n",
      "------------------------------\n",
      "The layer is Dense(32, 10)\n",
      "The size of the input layer is (32, 1000)\n",
      "The size of the output layer is (10, 1000)\n",
      "------------------------------\n",
      "The layer is softmax\n",
      "The size of the input layer is (10, 1000)\n",
      "The size of the output layer is (10, 1000)\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "layers = [layer1, layer2, layer3, layer4, layer5, layer6, layer7]\n",
    "x = x_train\n",
    "\n",
    "for l in layers\n",
    "    println(\"The layer is $l\")\n",
    "    println(\"The size of the input layer is $(size(x))\")\n",
    "    x = l(x)\n",
    "    println(\"The size of the output layer is $(size(x))\")\n",
    "    println(\"-\"^30)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,)\n",
      "(0,)\n",
      "EPOCH NUMBER: 1\n",
      "Training Loss is 0.003175072\n",
      "Validation Loss is 0.4989194\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 2\n",
      "Training Loss is 0.0033194362\n",
      "Validation Loss is 0.5593905\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 3\n",
      "Training Loss is 0.0012674299\n",
      "Validation Loss is 0.4932403\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 4\n",
      "Training Loss is 0.0013219134\n",
      "Validation Loss is 0.49928126\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 5\n",
      "Training Loss is 0.0010939803\n",
      "Validation Loss is 0.5000735\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 6\n",
      "Training Loss is 0.0008220038\n",
      "Validation Loss is 0.5133418\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 7\n",
      "Training Loss is 0.00069804705\n",
      "Validation Loss is 0.5000725\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 8\n",
      "Training Loss is 0.00065004907\n",
      "Validation Loss is 0.48983467\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 9\n",
      "Training Loss is 0.0005920033\n",
      "Validation Loss is 0.4962155\n",
      "--------------------------------------------------\n",
      "EPOCH NUMBER: 10\n",
      "Training Loss is 0.0005609194\n",
      "Validation Loss is 0.49872658\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = Chain(layer1, layer2, layer3, layer4, layer5,layer6, layer7)\n",
    "L(x,y) = Flux.crossentropy(model(x),y)\n",
    "opt = Flux.Optimise.ADAM()\n",
    "ps = Flux.params(model)\n",
    "training_loss = Vector{Float64}()\n",
    "validation_loss = Vector{Float64}()\n",
    "\n",
    "training_accuracy= Vector{Float64}()\n",
    "validation_accuracy = Vector{Float64}()\n",
    "\n",
    "println(size(training_loss))\n",
    "println(size(validation_loss))\n",
    "\n",
    "total_step = 10\n",
    "\n",
    "for step in 1:total_step\n",
    "    Flux.train!(L,ps,train_data,opt)\n",
    "    println(\"EPOCH NUMBER: $step\")\n",
    "    println(\"Training Loss is $(L(x_train,y_train))\")\n",
    "    println(\"Validation Loss is $(L(x_test,y_test))\")\n",
    "    println(\"-\"^50)\n",
    "    push!(training_loss,L(x_train,y_train))\n",
    "    push!(validation_loss,L(x_test,y_test))\n",
    "    \n",
    "    push!(training_accuracy,L(x_train,y_train))\n",
    "    push!(validation_loss,L(x_test,y_test))\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "println(size(training_loss))\n",
    "println(size(validation_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip900\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip900)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip901\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip900)\" d=\"\n",
       "M148.334 1486.45 L2352.76 1486.45 L2352.76 47.2441 L148.334 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip902\">\n",
       "    <rect x=\"148\" y=\"47\" width=\"2205\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip902)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  441.795,1486.45 441.795,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip902)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  903.938,1486.45 903.938,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip902)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1366.08,1486.45 1366.08,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip902)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1828.22,1486.45 1828.22,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip902)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2290.37,1486.45 2290.37,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip900)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  148.334,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip900)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  441.795,1486.45 441.795,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip900)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  903.938,1486.45 903.938,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip900)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1366.08,1486.45 1366.08,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip900)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1828.22,1486.45 1828.22,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip900)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2290.37,1486.45 2290.37,1469.18 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip900)\" d=\"M 0 0 M436.448 1543.18 L452.767 1543.18 L452.767 1547.12 L430.823 1547.12 L430.823 1543.18 Q433.485 1540.43 438.068 1535.8 Q442.675 1531.15 443.855 1529.81 Q446.101 1527.28 446.98 1525.55 Q447.883 1523.79 447.883 1522.1 Q447.883 1519.34 445.939 1517.61 Q444.017 1515.87 440.915 1515.87 Q438.716 1515.87 436.263 1516.63 Q433.832 1517.4 431.054 1518.95 L431.054 1514.23 Q433.878 1513.09 436.332 1512.51 Q438.786 1511.93 440.823 1511.93 Q446.193 1511.93 449.388 1514.62 Q452.582 1517.31 452.582 1521.8 Q452.582 1523.93 451.772 1525.85 Q450.985 1527.74 448.878 1530.34 Q448.3 1531.01 445.198 1534.23 Q442.096 1537.42 436.448 1543.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip900)\" d=\"M 0 0 M906.947 1516.63 L895.142 1535.08 L906.947 1535.08 L906.947 1516.63 M905.72 1512.56 L911.6 1512.56 L911.6 1535.08 L916.531 1535.08 L916.531 1538.97 L911.6 1538.97 L911.6 1547.12 L906.947 1547.12 L906.947 1538.97 L891.345 1538.97 L891.345 1534.46 L905.72 1512.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip900)\" d=\"M 0 0 M1366.49 1527.98 Q1363.34 1527.98 1361.49 1530.13 Q1359.66 1532.28 1359.66 1536.03 Q1359.66 1539.76 1361.49 1541.93 Q1363.34 1544.09 1366.49 1544.09 Q1369.63 1544.09 1371.46 1541.93 Q1373.31 1539.76 1373.31 1536.03 Q1373.31 1532.28 1371.46 1530.13 Q1369.63 1527.98 1366.49 1527.98 M1375.77 1513.32 L1375.77 1517.58 Q1374.01 1516.75 1372.2 1516.31 Q1370.42 1515.87 1368.66 1515.87 Q1364.03 1515.87 1361.58 1519 Q1359.15 1522.12 1358.8 1528.44 Q1360.17 1526.43 1362.23 1525.36 Q1364.29 1524.27 1366.76 1524.27 Q1371.97 1524.27 1374.98 1527.44 Q1378.01 1530.59 1378.01 1536.03 Q1378.01 1541.36 1374.87 1544.57 Q1371.72 1547.79 1366.49 1547.79 Q1360.49 1547.79 1357.32 1543.21 Q1354.15 1538.6 1354.15 1529.87 Q1354.15 1521.68 1358.04 1516.82 Q1361.93 1511.93 1368.48 1511.93 Q1370.24 1511.93 1372.02 1512.28 Q1373.82 1512.63 1375.77 1513.32 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip900)\" d=\"M 0 0 M1828.22 1530.71 Q1824.89 1530.71 1822.97 1532.49 Q1821.07 1534.27 1821.07 1537.4 Q1821.07 1540.52 1822.97 1542.31 Q1824.89 1544.09 1828.22 1544.09 Q1831.56 1544.09 1833.48 1542.31 Q1835.4 1540.5 1835.4 1537.4 Q1835.4 1534.27 1833.48 1532.49 Q1831.58 1530.71 1828.22 1530.71 M1823.55 1528.72 Q1820.54 1527.98 1818.85 1525.92 Q1817.18 1523.86 1817.18 1520.89 Q1817.18 1516.75 1820.12 1514.34 Q1823.08 1511.93 1828.22 1511.93 Q1833.39 1511.93 1836.33 1514.34 Q1839.27 1516.75 1839.27 1520.89 Q1839.27 1523.86 1837.58 1525.92 Q1835.91 1527.98 1832.92 1528.72 Q1836.3 1529.5 1838.18 1531.8 Q1840.08 1534.09 1840.08 1537.4 Q1840.08 1542.42 1837 1545.11 Q1833.94 1547.79 1828.22 1547.79 Q1822.51 1547.79 1819.43 1545.11 Q1816.37 1542.42 1816.37 1537.4 Q1816.37 1534.09 1818.27 1531.8 Q1820.17 1529.5 1823.55 1528.72 M1821.83 1521.33 Q1821.83 1524.02 1823.5 1525.52 Q1825.19 1527.03 1828.22 1527.03 Q1831.23 1527.03 1832.92 1525.52 Q1834.64 1524.02 1834.64 1521.33 Q1834.64 1518.65 1832.92 1517.14 Q1831.23 1515.64 1828.22 1515.64 Q1825.19 1515.64 1823.5 1517.14 Q1821.83 1518.65 1821.83 1521.33 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip900)\" d=\"M 0 0 M2267.24 1543.18 L2274.88 1543.18 L2274.88 1516.82 L2266.57 1518.49 L2266.57 1514.23 L2274.83 1512.56 L2279.51 1512.56 L2279.51 1543.18 L2287.15 1543.18 L2287.15 1547.12 L2267.24 1547.12 L2267.24 1543.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip900)\" d=\"M 0 0 M2302.22 1515.64 Q2298.61 1515.64 2296.78 1519.2 Q2294.97 1522.75 2294.97 1529.87 Q2294.97 1536.98 2296.78 1540.55 Q2298.61 1544.09 2302.22 1544.09 Q2305.85 1544.09 2307.66 1540.55 Q2309.49 1536.98 2309.49 1529.87 Q2309.49 1522.75 2307.66 1519.2 Q2305.85 1515.64 2302.22 1515.64 M2302.22 1511.93 Q2308.03 1511.93 2311.08 1516.54 Q2314.16 1521.12 2314.16 1529.87 Q2314.16 1538.6 2311.08 1543.21 Q2308.03 1547.79 2302.22 1547.79 Q2296.41 1547.79 2293.33 1543.21 Q2290.27 1538.6 2290.27 1529.87 Q2290.27 1521.12 2293.33 1516.54 Q2296.41 1511.93 2302.22 1511.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip902)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  148.334,1447.08 2352.76,1447.08 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip902)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  148.334,1204.12 2352.76,1204.12 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip902)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  148.334,961.156 2352.76,961.156 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip902)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  148.334,718.195 2352.76,718.195 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip902)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  148.334,475.233 2352.76,475.233 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip902)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  148.334,232.272 2352.76,232.272 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip900)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  148.334,1486.45 148.334,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip900)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  148.334,1447.08 174.787,1447.08 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip900)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  148.334,1204.12 174.787,1204.12 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip900)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  148.334,961.156 174.787,961.156 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip900)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  148.334,718.195 174.787,718.195 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip900)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  148.334,475.233 174.787,475.233 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip900)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  148.334,232.272 174.787,232.272 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip900)\" d=\"M 0 0 M63.4226 1432.88 Q59.8115 1432.88 57.9828 1436.44 Q56.1773 1439.98 56.1773 1447.11 Q56.1773 1454.22 57.9828 1457.78 Q59.8115 1461.33 63.4226 1461.33 Q67.0569 1461.33 68.8624 1457.78 Q70.6911 1454.22 70.6911 1447.11 Q70.6911 1439.98 68.8624 1436.44 Q67.0569 1432.88 63.4226 1432.88 M63.4226 1429.17 Q69.2328 1429.17 72.2883 1433.78 Q75.367 1438.36 75.367 1447.11 Q75.367 1455.84 72.2883 1460.45 Q69.2328 1465.03 63.4226 1465.03 Q57.6125 1465.03 54.5338 1460.45 Q51.4782 1455.84 51.4782 1447.11 Q51.4782 1438.36 54.5338 1433.78 Q57.6125 1429.17 63.4226 1429.17 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip900)\" d=\"M 0 0 M80.4364 1458.48 L85.3206 1458.48 L85.3206 1464.36 L80.4364 1464.36 L80.4364 1458.48 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip900)\" d=\"M 0 0 M100.39 1432.88 Q96.7789 1432.88 94.9502 1436.44 Q93.1447 1439.98 93.1447 1447.11 Q93.1447 1454.22 94.9502 1457.78 Q96.7789 1461.33 100.39 1461.33 Q104.024 1461.33 105.83 1457.78 Q107.658 1454.22 107.658 1447.11 Q107.658 1439.98 105.83 1436.44 Q104.024 1432.88 100.39 1432.88 M100.39 1429.17 Q106.2 1429.17 109.256 1433.78 Q112.334 1438.36 112.334 1447.11 Q112.334 1455.84 109.256 1460.45 Q106.2 1465.03 100.39 1465.03 Q94.5799 1465.03 91.5012 1460.45 Q88.4456 1455.84 88.4456 1447.11 Q88.4456 1438.36 91.5012 1433.78 Q94.5799 1429.17 100.39 1429.17 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip900)\" d=\"M 0 0 M64.6495 1189.92 Q61.0384 1189.92 59.2097 1193.48 Q57.4041 1197.02 57.4041 1204.15 Q57.4041 1211.26 59.2097 1214.82 Q61.0384 1218.36 64.6495 1218.36 Q68.2837 1218.36 70.0892 1214.82 Q71.9179 1211.26 71.9179 1204.15 Q71.9179 1197.02 70.0892 1193.48 Q68.2837 1189.92 64.6495 1189.92 M64.6495 1186.21 Q70.4596 1186.21 73.5152 1190.82 Q76.5938 1195.4 76.5938 1204.15 Q76.5938 1212.88 73.5152 1217.49 Q70.4596 1222.07 64.6495 1222.07 Q58.8393 1222.07 55.7606 1217.49 Q52.7051 1212.88 52.7051 1204.15 Q52.7051 1195.4 55.7606 1190.82 Q58.8393 1186.21 64.6495 1186.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip900)\" d=\"M 0 0 M81.6633 1215.52 L86.5475 1215.52 L86.5475 1221.4 L81.6633 1221.4 L81.6633 1215.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip900)\" d=\"M 0 0 M92.4271 1217.46 L100.066 1217.46 L100.066 1191.1 L91.7558 1192.76 L91.7558 1188.5 L100.02 1186.84 L104.696 1186.84 L104.696 1217.46 L112.334 1217.46 L112.334 1221.4 L92.4271 1221.4 L92.4271 1217.46 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip900)\" d=\"M 0 0 M65.0198 946.955 Q61.4087 946.955 59.58 950.519 Q57.7745 954.061 57.7745 961.191 Q57.7745 968.297 59.58 971.862 Q61.4087 975.404 65.0198 975.404 Q68.6541 975.404 70.4596 971.862 Q72.2883 968.297 72.2883 961.191 Q72.2883 954.061 70.4596 950.519 Q68.6541 946.955 65.0198 946.955 M65.0198 943.251 Q70.83 943.251 73.8855 947.857 Q76.9642 952.441 76.9642 961.191 Q76.9642 969.917 73.8855 974.524 Q70.83 979.107 65.0198 979.107 Q59.2097 979.107 56.131 974.524 Q53.0754 969.917 53.0754 961.191 Q53.0754 952.441 56.131 947.857 Q59.2097 943.251 65.0198 943.251 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip900)\" d=\"M 0 0 M82.0336 972.556 L86.9179 972.556 L86.9179 978.436 L82.0336 978.436 L82.0336 972.556 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip900)\" d=\"M 0 0 M96.015 974.501 L112.334 974.501 L112.334 978.436 L90.3901 978.436 L90.3901 974.501 Q93.0521 971.746 97.6354 967.117 Q102.242 962.464 103.422 961.121 Q105.668 958.598 106.547 956.862 Q107.45 955.103 107.45 953.413 Q107.45 950.658 105.506 948.922 Q103.584 947.186 100.483 947.186 Q98.2835 947.186 95.8298 947.95 Q93.3993 948.714 90.6215 950.265 L90.6215 945.543 Q93.4456 944.408 95.8993 943.83 Q98.353 943.251 100.39 943.251 Q105.76 943.251 108.955 945.936 Q112.149 948.621 112.149 953.112 Q112.149 955.242 111.339 957.163 Q110.552 959.061 108.446 961.654 Q107.867 962.325 104.765 965.543 Q101.663 968.737 96.015 974.501 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip900)\" d=\"M 0 0 M64.0708 703.993 Q60.4597 703.993 58.631 707.558 Q56.8254 711.1 56.8254 718.229 Q56.8254 725.336 58.631 728.901 Q60.4597 732.442 64.0708 732.442 Q67.705 732.442 69.5105 728.901 Q71.3392 725.336 71.3392 718.229 Q71.3392 711.1 69.5105 707.558 Q67.705 703.993 64.0708 703.993 M64.0708 700.29 Q69.8809 700.29 72.9365 704.896 Q76.0151 709.479 76.0151 718.229 Q76.0151 726.956 72.9365 731.563 Q69.8809 736.146 64.0708 736.146 Q58.2606 736.146 55.1819 731.563 Q52.1264 726.956 52.1264 718.229 Q52.1264 709.479 55.1819 704.896 Q58.2606 700.29 64.0708 700.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip900)\" d=\"M 0 0 M81.0846 729.595 L85.9688 729.595 L85.9688 735.475 L81.0846 735.475 L81.0846 729.595 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip900)\" d=\"M 0 0 M105.205 716.841 Q108.561 717.558 110.436 719.827 Q112.334 722.095 112.334 725.428 Q112.334 730.544 108.816 733.345 Q105.297 736.146 98.8159 736.146 Q96.64 736.146 94.3252 735.706 Q92.0336 735.29 89.5799 734.433 L89.5799 729.919 Q91.5243 731.053 93.8391 731.632 Q96.1539 732.211 98.6771 732.211 Q103.075 732.211 105.367 730.475 Q107.682 728.739 107.682 725.428 Q107.682 722.373 105.529 720.66 Q103.399 718.924 99.5798 718.924 L95.5521 718.924 L95.5521 715.081 L99.765 715.081 Q103.214 715.081 105.043 713.716 Q106.871 712.327 106.871 709.734 Q106.871 707.072 104.973 705.66 Q103.098 704.225 99.5798 704.225 Q97.6585 704.225 95.4595 704.642 Q93.2604 705.058 90.6215 705.938 L90.6215 701.771 Q93.2836 701.03 95.5984 700.66 Q97.9363 700.29 99.9965 700.29 Q105.321 700.29 108.422 702.72 Q111.524 705.128 111.524 709.248 Q111.524 712.118 109.881 714.109 Q108.237 716.077 105.205 716.841 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip900)\" d=\"M 0 0 M62.9365 461.032 Q59.3254 461.032 57.4967 464.597 Q55.6912 468.139 55.6912 475.268 Q55.6912 482.375 57.4967 485.939 Q59.3254 489.481 62.9365 489.481 Q66.5707 489.481 68.3763 485.939 Q70.205 482.375 70.205 475.268 Q70.205 468.139 68.3763 464.597 Q66.5707 461.032 62.9365 461.032 M62.9365 457.328 Q68.7467 457.328 71.8022 461.935 Q74.8809 466.518 74.8809 475.268 Q74.8809 483.995 71.8022 488.601 Q68.7467 493.185 62.9365 493.185 Q57.1264 493.185 54.0477 488.601 Q50.9921 483.995 50.9921 475.268 Q50.9921 466.518 54.0477 461.935 Q57.1264 457.328 62.9365 457.328 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip900)\" d=\"M 0 0 M79.9503 486.634 L84.8345 486.634 L84.8345 492.513 L79.9503 492.513 L79.9503 486.634 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip900)\" d=\"M 0 0 M102.751 462.027 L90.9456 480.476 L102.751 480.476 L102.751 462.027 M101.524 457.953 L107.404 457.953 L107.404 480.476 L112.334 480.476 L112.334 484.365 L107.404 484.365 L107.404 492.513 L102.751 492.513 L102.751 484.365 L87.1493 484.365 L87.1493 479.851 L101.524 457.953 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip900)\" d=\"M 0 0 M64.418 218.071 Q60.8069 218.071 58.9782 221.636 Q57.1726 225.177 57.1726 232.307 Q57.1726 239.413 58.9782 242.978 Q60.8069 246.52 64.418 246.52 Q68.0522 246.52 69.8578 242.978 Q71.6865 239.413 71.6865 232.307 Q71.6865 225.177 69.8578 221.636 Q68.0522 218.071 64.418 218.071 M64.418 214.367 Q70.2281 214.367 73.2837 218.974 Q76.3624 223.557 76.3624 232.307 Q76.3624 241.034 73.2837 245.64 Q70.2281 250.223 64.418 250.223 Q58.6078 250.223 55.5291 245.64 Q52.4736 241.034 52.4736 232.307 Q52.4736 223.557 55.5291 218.974 Q58.6078 214.367 64.418 214.367 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip900)\" d=\"M 0 0 M81.4318 243.673 L86.316 243.673 L86.316 249.552 L81.4318 249.552 L81.4318 243.673 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip900)\" d=\"M 0 0 M91.4317 214.992 L109.788 214.992 L109.788 218.927 L95.7141 218.927 L95.7141 227.399 Q96.7326 227.052 97.7511 226.89 Q98.7696 226.705 99.7882 226.705 Q105.575 226.705 108.955 229.876 Q112.334 233.048 112.334 238.464 Q112.334 244.043 108.862 247.145 Q105.39 250.223 99.0706 250.223 Q96.8947 250.223 94.6262 249.853 Q92.3808 249.483 89.9734 248.742 L89.9734 244.043 Q92.0567 245.177 94.2789 245.733 Q96.5011 246.288 98.978 246.288 Q102.983 246.288 105.321 244.182 Q107.658 242.075 107.658 238.464 Q107.658 234.853 105.321 232.747 Q102.983 230.64 98.978 230.64 Q97.103 230.64 95.228 231.057 Q93.3762 231.474 91.4317 232.353 L91.4317 214.992 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip902)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  210.724,1439.36 441.795,1439.01 672.867,1444 903.938,1443.87 1135.01,1444.42 1366.08,1445.08 1597.15,1445.38 1828.22,1445.5 2059.3,1445.64 2290.37,1445.72 \n",
       "  \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip902)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  210.724,234.898 441.795,87.9763 672.867,248.696 903.938,234.018 1135.01,232.094 1366.08,199.857 1597.15,232.096 1828.22,256.97 2059.3,241.467 2290.37,235.366 \n",
       "  \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip900)\" d=\"\n",
       "M1983.32 276.658 L2279.28 276.658 L2279.28 95.2176 L1983.32 95.2176  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip900)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1983.32,276.658 2279.28,276.658 2279.28,95.2176 1983.32,95.2176 1983.32,276.658 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip900)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2007.81,155.698 2154.78,155.698 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip900)\" d=\"M 0 0 M2193.11 175.385 Q2191.31 180.015 2189.59 181.427 Q2187.88 182.839 2185.01 182.839 L2181.61 182.839 L2181.61 179.274 L2184.11 179.274 Q2185.87 179.274 2186.84 178.44 Q2187.81 177.607 2188.99 174.505 L2189.76 172.561 L2179.27 147.052 L2183.78 147.052 L2191.89 167.329 L2199.99 147.052 L2204.5 147.052 L2193.11 175.385 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip900)\" d=\"M 0 0 M2210.38 169.042 L2218.02 169.042 L2218.02 142.677 L2209.71 144.343 L2209.71 140.084 L2217.97 138.418 L2222.65 138.418 L2222.65 169.042 L2230.29 169.042 L2230.29 172.978 L2210.38 172.978 L2210.38 169.042 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip900)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2007.81,216.178 2154.78,216.178 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip900)\" d=\"M 0 0 M2193.11 235.865 Q2191.31 240.495 2189.59 241.907 Q2187.88 243.319 2185.01 243.319 L2181.61 243.319 L2181.61 239.754 L2184.11 239.754 Q2185.87 239.754 2186.84 238.92 Q2187.81 238.087 2188.99 234.985 L2189.76 233.041 L2179.27 207.532 L2183.78 207.532 L2191.89 227.809 L2199.99 207.532 L2204.5 207.532 L2193.11 235.865 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip900)\" d=\"M 0 0 M2213.6 229.522 L2229.92 229.522 L2229.92 233.458 L2207.97 233.458 L2207.97 229.522 Q2210.64 226.768 2215.22 222.138 Q2219.83 217.485 2221.01 216.143 Q2223.25 213.62 2224.13 211.884 Q2225.03 210.124 2225.03 208.435 Q2225.03 205.68 2223.09 203.944 Q2221.17 202.208 2218.07 202.208 Q2215.87 202.208 2213.41 202.972 Q2210.98 203.735 2208.2 205.286 L2208.2 200.564 Q2211.03 199.43 2213.48 198.851 Q2215.94 198.273 2217.97 198.273 Q2223.34 198.273 2226.54 200.958 Q2229.73 203.643 2229.73 208.134 Q2229.73 210.263 2228.92 212.185 Q2228.14 214.083 2226.03 216.675 Q2225.45 217.347 2222.35 220.564 Q2219.25 223.759 2213.6 229.522 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(1:total_step,loss,legend=true)\n",
    "# plot!(1:total_step,validation_loss,legend=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = hcat(training_loss,validation_loss)\n",
    "size(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28×28×60000 Array{Int64,3}:\n",
       "[:, :, 1] =\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " ⋮              ⋮              ⋮        ⋱              ⋮              ⋮     \n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       "\n",
       "[:, :, 2] =\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " ⋮              ⋮              ⋮        ⋱              ⋮              ⋮     \n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       "\n",
       "[:, :, 3] =\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " ⋮              ⋮              ⋮        ⋱              ⋮              ⋮     \n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       "\n",
       "...\n",
       "\n",
       "[:, :, 59998] =\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " ⋮              ⋮              ⋮        ⋱              ⋮              ⋮     \n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       "\n",
       "[:, :, 59999] =\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " ⋮              ⋮              ⋮        ⋱              ⋮              ⋮     \n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       "\n",
       "[:, :, 60000] =\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " ⋮              ⋮              ⋮        ⋱              ⋮              ⋮     \n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1  …  1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1\n",
       " 1  1  1  1  1  1  1  1  1  1  1  1  1     1  1  1  1  1  1  1  1  1  1  1  1"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "length.(x_train_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
